{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Emotion Recognition in Greek Speech Using Wav2Vec 2.0","metadata":{"id":"5fbCls1d2yBs"}},{"cell_type":"markdown","source":"**Wav2Vec 2.0** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2`s ability to learn speech representations that are useful across multiple languages.\n\nSimilar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http://jalammar.github.io/illustrated-bert/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.\n\n![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)\n\nThe authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf).","metadata":{"id":"sp37lZOV2042"}},{"cell_type":"markdown","source":"During fine-tuning week hosted by HuggingFace, more than 300 people participated in tuning XLSR-Wav2Vec2's pretrained on low-resources ASR dataset for more than 50 languages. This model is fine-tuned using [Connectionist Temporal Classification](https://distill.pub/2017/ctc/) (CTC), an algorithm used to train neural networks for sequence-to-sequence problems and mainly in Automatic Speech Recognition and handwriting recognition. Follow this [notebook](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLSR_Wav2Vec2_on_Turkish_ASR_with_%F0%9F%A4%97_Transformers.ipynb#scrollTo=Gx9OdDYrCtQ1) for more information about XLSR-Wav2Vec2 fine-tuning.\n\nThis model was shown significant results in many low-resources languages. You can see the [competition board](https://paperswithcode.com/dataset/common-voice) or even testing the models from the [HuggingFace hub](https://huggingface.co/models?filter=xlsr-fine-tuning-week). \n\n\nIn this notebook, we will go through how to use this model to recognize the emotional aspects of speech in a language (or even as a general view using for every classification problem). Before going any further, we need to install some handy packages and define some enviroment values.","metadata":{"id":"y0xJwDkA3QQR"}},{"cell_type":"code","source":"%%capture\n\n# !pip install git+https://github.com/huggingface/datasets.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install datasets\n!pip install jiwer\n!pip install torchaudio\n!pip install librosa==0.8.1\n!pip show pyarrow\n!pip install gdown\n\n# Monitor the training process\n!pip install wandb","metadata":{"id":"TYsw43ULuXC_","execution":{"iopub.status.busy":"2023-04-06T14:34:52.475243Z","iopub.execute_input":"2023-04-06T14:34:52.476005Z","iopub.status.idle":"2023-04-06T14:36:51.078929Z","shell.execute_reply.started":"2023-04-06T14:34:52.475975Z","shell.execute_reply":"2023-04-06T14:36:51.077568Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torchaudio\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport sys","metadata":{"id":"pFSqZ0jwCMSv","execution":{"iopub.status.busy":"2023-04-06T14:36:51.081845Z","iopub.execute_input":"2023-04-06T14:36:51.083018Z","iopub.status.idle":"2023-04-06T14:36:53.512351Z","shell.execute_reply.started":"2023-04-06T14:36:51.082971Z","shell.execute_reply":"2023-04-06T14:36:53.511141Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"master_csvfile = \"../input/podcastfillersfull/PodcastFillers/metadata/PodcastFillers.csv\"\nclip_folder = \"../input/podcastfillersfull/PodcastFillers/audio/clip_wav_unpadded\"","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:46:03.942825Z","iopub.execute_input":"2023-04-06T14:46:03.944382Z","iopub.status.idle":"2023-04-06T14:46:03.949748Z","shell.execute_reply.started":"2023-04-06T14:46:03.944339Z","shell.execute_reply":"2023-04-06T14:46:03.948669Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = []\nevent_df = pd.read_csv(master_csvfile)\nfor i, event in tqdm(event_df.iterrows()):\n\n    clip_subset = event[\"clip_split_subset\"]\n    label = event['label_consolidated_vocab']\n\n    tar_folder = os.path.join(clip_folder, clip_subset)\n    tar_filepath = os.path.join(tar_folder, event[\"clip_name\"])\n\n    if not os.path.exists(tar_filepath):\n        continue\n    \n    data.append({\n        \"subset\": clip_subset,\n        \"path\": tar_filepath,\n        \"label\": label\n    })","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:46:04.103094Z","iopub.execute_input":"2023-04-06T14:46:04.103747Z","iopub.status.idle":"2023-04-06T14:47:01.519660Z","shell.execute_reply.started":"2023-04-06T14:46:04.103710Z","shell.execute_reply":"2023-04-06T14:47:01.517937Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"85803it [00:56, 1507.07it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.head()","metadata":{"id":"10b89ZpLDqx9","outputId":"4228d11b-e1cb-4f91-b8e0-1994167c0b22","execution":{"iopub.status.busy":"2023-04-06T14:47:01.521582Z","iopub.execute_input":"2023-04-06T14:47:01.522429Z","iopub.status.idle":"2023-04-06T14:47:01.549301Z","shell.execute_reply.started":"2023-04-06T14:47:01.522383Z","shell.execute_reply":"2023-04-06T14:47:01.548223Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       subset                                               path   label\n0       extra  ../input/podcastfillersfull/PodcastFillers/aud...    None\n1       train  ../input/podcastfillersfull/PodcastFillers/aud...   Music\n2  validation  ../input/podcastfillersfull/PodcastFillers/aud...      Uh\n3        test  ../input/podcastfillersfull/PodcastFillers/aud...  Breath\n4       train  ../input/podcastfillersfull/PodcastFillers/aud...   Music","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subset</th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>extra</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Music</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>validation</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Uh</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Breath</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Music</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:01.551030Z","iopub.execute_input":"2023-04-06T14:47:01.551972Z","iopub.status.idle":"2023-04-06T14:47:01.580013Z","shell.execute_reply.started":"2023-04-06T14:47:01.551930Z","shell.execute_reply":"2023-04-06T14:47:01.578982Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       subset                                               path  label\ncount   13662                                              13662  13662\nunique      4                                              13662      7\ntop     train  ../input/podcastfillersfull/PodcastFillers/aud...     Um\nfreq    10375                                                  1   3651","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subset</th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>13662</td>\n      <td>13662</td>\n      <td>13662</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>4</td>\n      <td>13662</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>train</td>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Um</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>10375</td>\n      <td>1</td>\n      <td>3651</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.iloc[0]['path']","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:01.582402Z","iopub.execute_input":"2023-04-06T14:47:01.583082Z","iopub.status.idle":"2023-04-06T14:47:01.590243Z","shell.execute_reply.started":"2023-04-06T14:47:01.583044Z","shell.execute_reply":"2023-04-06T14:47:01.589160Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'../input/podcastfillersfull/PodcastFillers/audio/clip_wav_unpadded/extra/00000.wav'"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's explore how many labels are in the dataset with what distribution.","metadata":{"id":"TWku4ra3Bp52"}},{"cell_type":"code","source":"print(\"Labels: \", df[\"label\"].unique())\nprint()\ndf.groupby(\"label\").count()[[\"path\"]]","metadata":{"id":"beNpKMh5xXmX","outputId":"171d276b-8be2-45fc-943c-3610a5c4e1b4","execution":{"iopub.status.busy":"2023-04-06T14:47:01.592191Z","iopub.execute_input":"2023-04-06T14:47:01.593164Z","iopub.status.idle":"2023-04-06T14:47:01.610841Z","shell.execute_reply.started":"2023-04-06T14:47:01.593123Z","shell.execute_reply":"2023-04-06T14:47:01.609784Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Labels:  ['None' 'Music' 'Uh' 'Breath' 'Um' 'Words' 'Laughter']\n\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          path\nlabel         \nBreath     953\nLaughter   940\nMusic      949\nNone      1319\nUh        2912\nUm        3651\nWords     2938","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Breath</th>\n      <td>953</td>\n    </tr>\n    <tr>\n      <th>Laughter</th>\n      <td>940</td>\n    </tr>\n    <tr>\n      <th>Music</th>\n      <td>949</td>\n    </tr>\n    <tr>\n      <th>None</th>\n      <td>1319</td>\n    </tr>\n    <tr>\n      <th>Uh</th>\n      <td>2912</td>\n    </tr>\n    <tr>\n      <th>Um</th>\n      <td>3651</td>\n    </tr>\n    <tr>\n      <th>Words</th>\n      <td>2938</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's display some random sample of the dataset and run it a couple of times to get a feeling for the audio and the emotional label.","metadata":{"id":"i2hwRai7BNrx"}},{"cell_type":"code","source":"import torchaudio\nimport librosa\nimport IPython.display as ipd\nimport numpy as np\n\n# idx = np.random.randint(0, len(df))\nidx = 5\nsample = df.iloc[idx]\npath = sample[\"path\"]\nlabel = sample[\"label\"]\n\n\nprint(f\"ID Location: {idx}\")\nprint(f\"      Label: {label}\")\nprint()\n\n# speech, sr = torchaudio.load(path)\n# speech = speech[0].numpy().squeeze()\n# speech = librosa.resample(np.asarray(speech), sr, 16_000)\n# ipd.Audio(data=np.asarray(speech), autoplay=True, rate=16000)","metadata":{"id":"DZaQ_sP5xkIX","outputId":"aa27d8ae-d95e-475f-eee7-e9e05f4cbe1b","execution":{"iopub.status.busy":"2023-04-06T14:47:01.612423Z","iopub.execute_input":"2023-04-06T14:47:01.613101Z","iopub.status.idle":"2023-04-06T14:47:02.583927Z","shell.execute_reply.started":"2023-04-06T14:47:01.613063Z","shell.execute_reply":"2023-04-06T14:47:02.582108Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ID Location: 5\n      Label: Um\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir content\n!mkdir content/data","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:02.585546Z","iopub.execute_input":"2023-04-06T14:47:02.586125Z","iopub.status.idle":"2023-04-06T14:47:04.488586Z","shell.execute_reply.started":"2023-04-06T14:47:02.586085Z","shell.execute_reply":"2023-04-06T14:47:04.487240Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:04.491820Z","iopub.execute_input":"2023-04-06T14:47:04.492502Z","iopub.status.idle":"2023-04-06T14:47:05.453352Z","shell.execute_reply.started":"2023-04-06T14:47:04.492466Z","shell.execute_reply":"2023-04-06T14:47:05.452149Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  content\n","output_type":"stream"}]},{"cell_type":"code","source":"save_path = \"content/data\"\n\n# train_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"emotion\"])\ntrain_data = []\ntest_data = []\nvalid_data = []\nextra_data = []\n# event_df = pd.read_csv(master_csvfile)\nfor i, event in tqdm(df.iterrows()):\n\n    if event['subset'] == 'train':\n        train_data.append({\n            'path': event['path'],\n            'label': event['label']\n        })\n        \n    elif event['subset'] == 'test':\n        test_data.append({\n            'path': event['path'],\n            'label': event['label']\n        })\n        \n    elif event['subset'] == 'validation':\n        valid_data.append({\n            'path': event['path'],\n            'label': event['label']\n        })\n        \n    elif event['subset'] == 'extra':\n        extra_data.append({\n            'path': event['path'],\n            'label': event['label']\n        })\n    else:\n        print(\"bad subset:\", event['subset'])\n","metadata":{"id":"mlim-044xtJN","outputId":"c8b2530e-91bd-40e1-e60c-086f1ff8431e","execution":{"iopub.status.busy":"2023-04-06T14:47:05.455083Z","iopub.execute_input":"2023-04-06T14:47:05.455457Z","iopub.status.idle":"2023-04-06T14:47:06.320684Z","shell.execute_reply.started":"2023-04-06T14:47:05.455413Z","shell.execute_reply":"2023-04-06T14:47:06.319614Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"13662it [00:00, 16060.61it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.DataFrame(train_data)\ntest_df = pd.DataFrame(test_data)\nvalid_df = pd.DataFrame(valid_data)\nextra_df = pd.DataFrame(extra_data)\n\ntrain_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\ntest_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\nvalid_df.to_csv(f\"{save_path}/valid.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\nextra_df.to_csv(f\"{save_path}/extra.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n\n\nprint(train_df.shape)\nprint(test_df.shape)\nprint(valid_df.shape)\nprint(extra_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:06.325333Z","iopub.execute_input":"2023-04-06T14:47:06.325621Z","iopub.status.idle":"2023-04-06T14:47:06.377116Z","shell.execute_reply.started":"2023-04-06T14:47:06.325593Z","shell.execute_reply":"2023-04-06T14:47:06.375889Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(10375, 2)\n(1568, 2)\n(400, 2)\n(1319, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Labels: \", df[\"label\"].unique())\nprint()\ntrain_df.groupby(\"label\").count()[[\"path\"]]","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:06.378495Z","iopub.execute_input":"2023-04-06T14:47:06.379136Z","iopub.status.idle":"2023-04-06T14:47:06.398415Z","shell.execute_reply.started":"2023-04-06T14:47:06.379097Z","shell.execute_reply":"2023-04-06T14:47:06.397500Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Labels:  ['None' 'Music' 'Uh' 'Breath' 'Um' 'Words' 'Laughter']\n\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"          path\nlabel         \nBreath     852\nLaughter   828\nMusic      792\nUh        2396\nUm        3011\nWords     2496","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Breath</th>\n      <td>852</td>\n    </tr>\n    <tr>\n      <th>Laughter</th>\n      <td>828</td>\n    </tr>\n    <tr>\n      <th>Music</th>\n      <td>792</td>\n    </tr>\n    <tr>\n      <th>Uh</th>\n      <td>2396</td>\n    </tr>\n    <tr>\n      <th>Um</th>\n      <td>3011</td>\n    </tr>\n    <tr>\n      <th>Words</th>\n      <td>2496</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:06.399700Z","iopub.execute_input":"2023-04-06T14:47:06.400020Z","iopub.status.idle":"2023-04-06T14:47:06.411500Z","shell.execute_reply.started":"2023-04-06T14:47:06.399991Z","shell.execute_reply":"2023-04-06T14:47:06.410320Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                   path   label\n0     ../input/podcastfillersfull/PodcastFillers/aud...  Breath\n1     ../input/podcastfillersfull/PodcastFillers/aud...   Words\n2     ../input/podcastfillersfull/PodcastFillers/aud...  Breath\n3     ../input/podcastfillersfull/PodcastFillers/aud...      Um\n4     ../input/podcastfillersfull/PodcastFillers/aud...      Um\n...                                                 ...     ...\n1563  ../input/podcastfillersfull/PodcastFillers/aud...      Um\n1564  ../input/podcastfillersfull/PodcastFillers/aud...   Words\n1565  ../input/podcastfillersfull/PodcastFillers/aud...      Uh\n1566  ../input/podcastfillersfull/PodcastFillers/aud...      Um\n1567  ../input/podcastfillersfull/PodcastFillers/aud...   Music\n\n[1568 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Breath</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Words</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Breath</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Um</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Um</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1563</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Um</td>\n    </tr>\n    <tr>\n      <th>1564</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Words</td>\n    </tr>\n    <tr>\n      <th>1565</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Uh</td>\n    </tr>\n    <tr>\n      <th>1566</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Um</td>\n    </tr>\n    <tr>\n      <th>1567</th>\n      <td>../input/podcastfillersfull/PodcastFillers/aud...</td>\n      <td>Music</td>\n    </tr>\n  </tbody>\n</table>\n<p>1568 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare Data for Training","metadata":{"id":"kcnD-d_rDElt"}},{"cell_type":"code","source":"# Loading the created dataset using datasets\nfrom datasets import load_dataset, load_metric\n\n\ndata_files = {\n    \"train\": \"content/data/train.csv\", \n    \"test\": \"content/data/test.csv\", \n    \"validation\": \"content/data/valid.csv\",\n    \"extra\": \"content/data/extra.csv\",\n}\n\ndataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\neval_dataset = dataset[\"validation\"]\nextra_dataset = dataset[\"extra\"]\n\n\nprint(train_dataset)\nprint(test_dataset)\nprint(eval_dataset)\nprint(extra_dataset)","metadata":{"id":"nnVfxQYDDIc6","outputId":"813a7f17-b823-4d2a-e298-734646242912","execution":{"iopub.status.busy":"2023-04-06T14:47:06.413134Z","iopub.execute_input":"2023-04-06T14:47:06.414312Z","iopub.status.idle":"2023-04-06T14:47:08.513455Z","shell.execute_reply.started":"2023-04-06T14:47:06.414273Z","shell.execute_reply":"2023-04-06T14:47:08.512405Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-2844d04970436f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450aa31a97f346f39a5bb1d9d33a531d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af0592e46ea4e24b87f537b6d70ad91"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-2844d04970436f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf0a166cf91343f28338093a734332f6"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['path', 'label'],\n    num_rows: 10375\n})\nDataset({\n    features: ['path', 'label'],\n    num_rows: 1568\n})\nDataset({\n    features: ['path', 'label'],\n    num_rows: 400\n})\nDataset({\n    features: ['path', 'label'],\n    num_rows: 1319\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"type(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:08.515062Z","iopub.execute_input":"2023-04-06T14:47:08.515744Z","iopub.status.idle":"2023-04-06T14:47:08.522988Z","shell.execute_reply.started":"2023-04-06T14:47:08.515704Z","shell.execute_reply":"2023-04-06T14:47:08.521935Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}]},{"cell_type":"code","source":"# We need to specify the input and output column\ninput_column = \"path\"\noutput_column = \"label\"","metadata":{"id":"rsOrQPBOEVOy","execution":{"iopub.status.busy":"2023-04-06T14:47:08.524751Z","iopub.execute_input":"2023-04-06T14:47:08.525431Z","iopub.status.idle":"2023-04-06T14:47:08.533563Z","shell.execute_reply.started":"2023-04-06T14:47:08.525393Z","shell.execute_reply":"2023-04-06T14:47:08.532456Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# we need to distinguish the unique labels in our SER dataset\nlabel_list = train_dataset.unique(output_column)\nlabel_list.sort()  # Let's sort it for determinism\nnum_labels = len(label_list)\nprint(f\"A classification problem with {num_labels} classes: {label_list}\")","metadata":{"id":"-gh7fQ1XEpC7","outputId":"354be843-a3ed-4dbf-af47-df935c90e9d2","execution":{"iopub.status.busy":"2023-04-06T14:47:08.535248Z","iopub.execute_input":"2023-04-06T14:47:08.536060Z","iopub.status.idle":"2023-04-06T14:47:08.545749Z","shell.execute_reply.started":"2023-04-06T14:47:08.536024Z","shell.execute_reply":"2023-04-06T14:47:08.544530Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"A classification problem with 6 classes: ['Breath', 'Laughter', 'Music', 'Uh', 'Um', 'Words']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In order to preprocess the audio into our classification model, we need to set up the relevant Wav2Vec2 assets regarding our language in this case `jonatasgrosman/wav2vec2-large-xlsr-53-russian` . To handle the context representations in any audio length we use a merge strategy plan (pooling mode) to concatenate that 3D representations into 2D representations.\n\nThere are three merge strategies `mean`, `sum`, and `max`. In this example, we achieved better results on the mean approach. In the following, we need to initiate the config and the feature extractor from the Dimitris model.","metadata":{"id":"4TkGYrVTFR6Y"}},{"cell_type":"code","source":"from transformers import AutoConfig, Wav2Vec2Processor","metadata":{"id":"rQrGaFohGzVo","execution":{"iopub.status.busy":"2023-04-06T14:47:08.547450Z","iopub.execute_input":"2023-04-06T14:47:08.548062Z","iopub.status.idle":"2023-04-06T14:47:09.585369Z","shell.execute_reply.started":"2023-04-06T14:47:08.548028Z","shell.execute_reply":"2023-04-06T14:47:09.584339Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\npooling_mode = \"mean\"","metadata":{"id":"9Y1adr7vFrq7","execution":{"iopub.status.busy":"2023-04-06T14:47:09.586956Z","iopub.execute_input":"2023-04-06T14:47:09.587604Z","iopub.status.idle":"2023-04-06T14:47:09.594532Z","shell.execute_reply.started":"2023-04-06T14:47:09.587564Z","shell.execute_reply":"2023-04-06T14:47:09.592299Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"id2label_ = {i: label for i, label in enumerate(label_list)}\nid2label_","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:11:38.562336Z","iopub.execute_input":"2023-04-06T16:11:38.562957Z","iopub.status.idle":"2023-04-06T16:11:38.572372Z","shell.execute_reply.started":"2023-04-06T16:11:38.562911Z","shell.execute_reply":"2023-04-06T16:11:38.571199Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"{0: 'Breath', 1: 'Laughter', 2: 'Music', 3: 'Uh', 4: 'Um', 5: 'Words'}"},"metadata":{}}]},{"cell_type":"code","source":"label_list","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:17:44.913369Z","iopub.execute_input":"2023-04-06T16:17:44.913756Z","iopub.status.idle":"2023-04-06T16:17:44.923603Z","shell.execute_reply.started":"2023-04-06T16:17:44.913721Z","shell.execute_reply":"2023-04-06T16:17:44.921729Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"['Breath', 'Laughter', 'Music', 'Uh', 'Um', 'Words']"},"metadata":{}}]},{"cell_type":"code","source":"# config\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id={label: i for i, label in enumerate(label_list)},\n    id2label={i: label for i, label in enumerate(label_list)},\n    finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)","metadata":{"id":"AZjDSmBRGqr6","outputId":"2286c25e-6d93-4355-8f69-e592fe15d5d4","execution":{"iopub.status.busy":"2023-04-06T14:47:09.596150Z","iopub.execute_input":"2023-04-06T14:47:09.596786Z","iopub.status.idle":"2023-04-06T14:47:10.112087Z","shell.execute_reply.started":"2023-04-06T14:47:09.596747Z","shell.execute_reply":"2023-04-06T14:47:10.111031Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea8faf59a464714b327b1afd0999a77"}},"metadata":{}}]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\nprint(f\"The target sampling rate: {target_sampling_rate}\")","metadata":{"id":"ZXVl9qW1Gw_-","outputId":"9dd51ab8-e5ef-4911-9a82-a0fa413ec33c","execution":{"iopub.status.busy":"2023-04-06T14:47:10.113484Z","iopub.execute_input":"2023-04-06T14:47:10.114436Z","iopub.status.idle":"2023-04-06T14:47:12.311773Z","shell.execute_reply.started":"2023-04-06T14:47:10.114397Z","shell.execute_reply":"2023-04-06T14:47:12.310665Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a7c1a105924245acf312e009866815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"363bdf32259e41d489586c3e63c68cd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d5e39837084484bd430ddfc6e4d9f9"}},"metadata":{}},{"name":"stdout","text":"The target sampling rate: 16000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{"id":"CbGuYgLqHXZg"}},{"cell_type":"markdown","source":"So far, we downloaded, loaded, and split the SER dataset into train and test sets. The instantiated our strategy configuration for using context representations in our classification problem SER. Now, we need to extract features from the audio path in context representation tensors and feed them into our classification model to determine the emotion in the speech.\n\nSince the audio file is saved in the `.wav` format, it is easy to use **[Librosa](https://librosa.org/doc/latest/index.html)** or others, but we suppose that the format may be in the `.mp3` format in case of generality. We found that the **[Torchaudio](https://pytorch.org/audio/stable/index.html)** library works best for reading in `.mp3` data.\n\nAn audio file usually stores both its values and the sampling rate with which the speech signal was digitalized. We want to store both in the dataset and write a **map(...)** function accordingly. Also, we need to handle the string labels into integers for our specific classification task in this case, the **single-label classification** you may want to use for your **regression** or even **multi-label classification**.","metadata":{"id":"qLk-eM1DFjtE"}},{"cell_type":"code","source":"def speech_file_to_array_fn(path):\n    speech_array, sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\ndef label_to_id(label, label_list):\n\n    if len(label_list) > 0:\n        return label_list.index(label) if label in label_list else -1\n\n    return label\n\ndef preprocess_function(examples):\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n\n    result = processor(speech_list, sampling_rate=target_sampling_rate)\n    result[\"labels\"] = list(target_list)\n\n    return result","metadata":{"id":"6UqlIV3uGxDA","execution":{"iopub.status.busy":"2023-04-06T14:47:12.313313Z","iopub.execute_input":"2023-04-06T14:47:12.314371Z","iopub.status.idle":"2023-04-06T14:47:12.323762Z","shell.execute_reply.started":"2023-04-06T14:47:12.314332Z","shell.execute_reply":"2023-04-06T14:47:12.322440Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    preprocess_function,\n    batch_size=100,\n    batched=True,\n    num_proc=4\n)\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=100,\n    batched=True,\n    num_proc=4\n)","metadata":{"id":"ioP8FfR2GxHi","outputId":"2bea4ed0-33d6-4183-9e6c-cbdf14072453","execution":{"iopub.status.busy":"2023-04-06T14:47:12.325658Z","iopub.execute_input":"2023-04-06T14:47:12.326097Z","iopub.status.idle":"2023-04-06T14:47:39.032860Z","shell.execute_reply.started":"2023-04-06T14:47:12.326059Z","shell.execute_reply":"2023-04-06T14:47:39.031623Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faabc1dd66da43b49a42088b3dbb3769"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1982571a874f028d03b1e2164b3d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0d68140ac14e10a7c79d4124303de4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af88536231154e78a024b74d2b5ed964"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n","output_type":"stream"},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6177442192b24acd8bf4b674e1512e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2753c822791d483e827871431a7dc097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de744908c614dc0b7c0f62af03db789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3729e63b7c0d41a6a1bc8b6723a8dd91"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n/opt/conda/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Great, now we've successfully read all the audio files, resampled the audio files to 16kHz, and mapped each audio to the corresponding label.","metadata":{"id":"HcrEgJO9Hmx7"}},{"cell_type":"markdown","source":"## Model\n\nBefore diving into the training part, we need to build our classification model based on the merge strategy. ","metadata":{"id":"QL7q6DfcH0Fs"}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom transformers.file_utils import ModelOutput\n\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","metadata":{"id":"uJZoC4T5HpuP","execution":{"iopub.status.busy":"2023-04-06T14:47:39.034776Z","iopub.execute_input":"2023-04-06T14:47:39.035146Z","iopub.status.idle":"2023-04-06T14:47:39.045182Z","shell.execute_reply.started":"2023-04-06T14:47:39.035105Z","shell.execute_reply":"2023-04-06T14:47:39.044167Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)\n\n\nclass Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.out_proj(x)\n        return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.pooling_mode = config.pooling_mode\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(\n            self,\n            hidden_states,\n            mode=\"mean\"\n    ):\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        else:\n            raise Exception(\n                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n        return outputs\n\n    def forward(\n            self,\n            input_values,\n            attention_mask=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            labels=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n#         print(\"hidden_states.shape: \", hidden_states.shape)\n        logits = self.classifier(hidden_states)\n#         print(\"logits: \", logits)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"id":"Fv62ShDsH5DZ","execution":{"iopub.status.busy":"2023-04-06T14:47:39.046820Z","iopub.execute_input":"2023-04-06T14:47:39.047179Z","iopub.status.idle":"2023-04-06T14:47:39.940373Z","shell.execute_reply.started":"2023-04-06T14:47:39.047135Z","shell.execute_reply":"2023-04-06T14:47:39.939009Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nThe data is processed so that we are ready to start setting up the training pipeline. We will make use of 🤗's [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n\n- Define a data collator. In contrast to most NLP models, XLSR-Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLSR-Wav2Vec2 requires a special padding data collator, which we will define below\n\n- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n\n- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n\n- Define the training configuration.\n\nAfter having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech.","metadata":{"id":"OrBrR1b7zvUL"}},{"cell_type":"markdown","source":"### Set-up Trainer\n\nLet's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81).\n\nWithout going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\nAnalogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss.","metadata":{"id":"Ji9-n1eUIKZc"}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\n\nimport transformers\nfrom transformers import Wav2Vec2Processor\n\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [feature[\"labels\"] for feature in features]\n\n        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n\n        return batch","metadata":{"id":"rkM0VLIwy903","execution":{"iopub.status.busy":"2023-04-06T14:47:39.942599Z","iopub.execute_input":"2023-04-06T14:47:39.943033Z","iopub.status.idle":"2023-04-06T14:47:39.957206Z","shell.execute_reply.started":"2023-04-06T14:47:39.942987Z","shell.execute_reply":"2023-04-06T14:47:39.955854Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:47:39.959369Z","iopub.execute_input":"2023-04-06T14:47:39.960148Z","iopub.status.idle":"2023-04-06T14:47:41.778416Z","shell.execute_reply.started":"2023-04-06T14:47:39.960106Z","shell.execute_reply":"2023-04-06T14:47:41.777327Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"id":"-YhJ0OA4RxQe","execution":{"iopub.status.busy":"2023-04-06T14:47:41.779958Z","iopub.execute_input":"2023-04-06T14:47:41.780438Z","iopub.status.idle":"2023-04-06T14:47:41.787927Z","shell.execute_reply.started":"2023-04-06T14:47:41.780398Z","shell.execute_reply":"2023-04-06T14:47:41.786931Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Next, the evaluation metric is defined. There are many pre-defined metrics for classification/regression problems, but in this case, we would continue with just **Accuracy** for classification and **MSE** for regression. You can define other metrics on your own.","metadata":{"id":"mYxy2IR-KcU2"}},{"cell_type":"code","source":"is_regression = False","metadata":{"id":"LL8I5MKvPnth","execution":{"iopub.status.busy":"2023-04-06T14:47:41.794081Z","iopub.execute_input":"2023-04-06T14:47:41.794374Z","iopub.status.idle":"2023-04-06T14:47:41.799638Z","shell.execute_reply.started":"2023-04-06T14:47:41.794347Z","shell.execute_reply":"2023-04-06T14:47:41.798424Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import EvalPrediction\n\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n\n    if is_regression:\n        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n    else:\n#         print(\"preds:\", preds)\n        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}","metadata":{"id":"XK26Z6IfR36K","execution":{"iopub.status.busy":"2023-04-06T14:47:41.801395Z","iopub.execute_input":"2023-04-06T14:47:41.802071Z","iopub.status.idle":"2023-04-06T14:47:41.810551Z","shell.execute_reply.started":"2023-04-06T14:47:41.802034Z","shell.execute_reply":"2023-04-06T14:47:41.809549Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Now, we can load the pretrained XLSR-Wav2Vec2 checkpoint into our classification model with a pooling strategy.","metadata":{"id":"ZsH_nKJdK28o"}},{"cell_type":"code","source":"model = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"id":"0Tl6iKAUR4EL","outputId":"7af3b6e2-dfc2-4e78-b326-010e32f86a94","execution":{"iopub.status.busy":"2023-04-06T14:47:41.812998Z","iopub.execute_input":"2023-04-06T14:47:41.813666Z","iopub.status.idle":"2023-04-06T14:48:40.975611Z","shell.execute_reply.started":"2023-04-06T14:47:41.813629Z","shell.execute_reply":"2023-04-06T14:48:40.974606Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8482e390ce81438eb23655a3b8595b65"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \nThus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.","metadata":{"id":"bqF4rNMzI1M5"}},{"cell_type":"code","source":"model.freeze_feature_extractor()","metadata":{"id":"KHMhxFGoR4Hb","execution":{"iopub.status.busy":"2023-04-06T14:48:40.977182Z","iopub.execute_input":"2023-04-06T14:48:40.977775Z","iopub.status.idle":"2023-04-06T14:48:40.983841Z","shell.execute_reply.started":"2023-04-06T14:48:40.977735Z","shell.execute_reply":"2023-04-06T14:48:40.982321Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"In a final step, we define all parameters related to training. \nTo give more explanation on some of the parameters:\n- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n\nFor more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n\n**Note**: If one wants to save the trained models in his/her google drive the commented-out `output_dir` can be used instead.","metadata":{"id":"n0HzBneBK84G"}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/content/wav2vec2-uh_um-recognition\",\n    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"steps\",\n    num_train_epochs=1.0,\n    fp16=True,\n    save_steps=200,\n    eval_steps=20,\n    logging_steps=20,\n    learning_rate=1e-4,\n    save_total_limit=1,\n)","metadata":{"id":"vUtWjldAI9-H","execution":{"iopub.status.busy":"2023-04-06T14:48:40.985977Z","iopub.execute_input":"2023-04-06T14:48:40.986318Z","iopub.status.idle":"2023-04-06T14:48:41.053607Z","shell.execute_reply.started":"2023-04-06T14:48:40.986278Z","shell.execute_reply":"2023-04-06T14:48:41.052662Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"For future use we can create our training script, we do it in a simple way. You can add more on you own.","metadata":{"id":"XAtuL0APLZSs"}},{"cell_type":"markdown","source":"Now, all instances can be passed to Trainer and we are ready to start training!","metadata":{"id":"Qv7Ju3qYJeJn"}},{"cell_type":"code","source":"type(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:48:41.054936Z","iopub.execute_input":"2023-04-06T14:48:41.055547Z","iopub.status.idle":"2023-04-06T14:48:41.062377Z","shell.execute_reply.started":"2023-04-06T14:48:41.055511Z","shell.execute_reply":"2023-04-06T14:48:41.061286Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"id":"nEFkfK45JYiZ","execution":{"iopub.status.busy":"2023-04-06T14:48:41.063772Z","iopub.execute_input":"2023-04-06T14:48:41.064436Z","iopub.status.idle":"2023-04-06T14:48:44.911718Z","shell.execute_reply.started":"2023-04-06T14:48:41.064397Z","shell.execute_reply":"2023-04-06T14:48:44.910683Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"id":"0gGLwJAOLtDg"}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"6M8bNvLLJnG1","outputId":"819249df-b341-4007-d6de-b8137dfa1651","execution":{"iopub.status.busy":"2023-04-06T14:48:44.913279Z","iopub.execute_input":"2023-04-06T14:48:44.913640Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230406_150639-93mi5ii8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sweethash/huggingface/runs/93mi5ii8' target=\"_blank\">betazoid-unimatrix-4</a></strong> to <a href='https://wandb.ai/sweethash/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sweethash/huggingface' target=\"_blank\">https://wandb.ai/sweethash/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sweethash/huggingface/runs/93mi5ii8' target=\"_blank\">https://wandb.ai/sweethash/huggingface/runs/93mi5ii8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='977' max='1297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 977/1297 10:20 < 03:23, 1.57 it/s, Epoch 0.75/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.502100</td>\n      <td>1.165604</td>\n      <td>0.585000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.365000</td>\n      <td>1.003593</td>\n      <td>0.590000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.337000</td>\n      <td>1.331674</td>\n      <td>0.542500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.305500</td>\n      <td>0.900754</td>\n      <td>0.740000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.034300</td>\n      <td>0.757725</td>\n      <td>0.722500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.090900</td>\n      <td>0.732453</td>\n      <td>0.747500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.309300</td>\n      <td>0.868702</td>\n      <td>0.727500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.052900</td>\n      <td>0.798538</td>\n      <td>0.755000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.028300</td>\n      <td>0.701075</td>\n      <td>0.770000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.035800</td>\n      <td>0.701215</td>\n      <td>0.737500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.971700</td>\n      <td>0.545608</td>\n      <td>0.830000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.945800</td>\n      <td>0.550598</td>\n      <td>0.830000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.835500</td>\n      <td>0.667638</td>\n      <td>0.762500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.001900</td>\n      <td>0.504681</td>\n      <td>0.845000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.883400</td>\n      <td>0.566406</td>\n      <td>0.827500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.876900</td>\n      <td>0.555933</td>\n      <td>0.832500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.932500</td>\n      <td>0.455484</td>\n      <td>0.857500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.914800</td>\n      <td>0.525704</td>\n      <td>0.820000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.897900</td>\n      <td>0.545458</td>\n      <td>0.822500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.904000</td>\n      <td>0.439229</td>\n      <td>0.860000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.835900</td>\n      <td>0.415453</td>\n      <td>0.862500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.822200</td>\n      <td>0.471949</td>\n      <td>0.845000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.845800</td>\n      <td>0.458082</td>\n      <td>0.847500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.848100</td>\n      <td>0.433025</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.014500</td>\n      <td>0.387095</td>\n      <td>0.882500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.021600</td>\n      <td>0.378727</td>\n      <td>0.867500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.868100</td>\n      <td>0.354589</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.743300</td>\n      <td>0.361594</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.793400</td>\n      <td>0.358162</td>\n      <td>0.897500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.850300</td>\n      <td>0.336509</td>\n      <td>0.897500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.754600</td>\n      <td>0.376417</td>\n      <td>0.877500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.837100</td>\n      <td>0.458911</td>\n      <td>0.860000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.756900</td>\n      <td>0.342603</td>\n      <td>0.895000</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.866800</td>\n      <td>0.325949</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.839200</td>\n      <td>0.329656</td>\n      <td>0.885000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.693600</td>\n      <td>0.345823</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.743100</td>\n      <td>0.375318</td>\n      <td>0.865000</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.737900</td>\n      <td>0.347994</td>\n      <td>0.890000</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.829800</td>\n      <td>0.320313</td>\n      <td>0.890000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.679100</td>\n      <td>0.306454</td>\n      <td>0.890000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.876700</td>\n      <td>0.297931</td>\n      <td>0.905000</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.824700</td>\n      <td>0.314328</td>\n      <td>0.885000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.724600</td>\n      <td>0.286868</td>\n      <td>0.902500</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.777000</td>\n      <td>0.311354</td>\n      <td>0.895000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.747400</td>\n      <td>0.285451</td>\n      <td>0.907500</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.762700</td>\n      <td>0.312081</td>\n      <td>0.907500</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.672500</td>\n      <td>0.267494</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.729800</td>\n      <td>0.274293</td>\n      <td>0.915000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:51:37.134890Z","iopub.execute_input":"2023-04-06T15:51:37.135592Z","iopub.status.idle":"2023-04-06T15:51:48.598882Z","shell.execute_reply.started":"2023-04-06T15:51:37.135554Z","shell.execute_reply":"2023-04-06T15:51:48.595281Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.9.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# https://drive.google.com/file/d/1WfnmYuP58qC_68dKIcYgqqbPc_Usm6yU/view?usp=share_link","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:52:34.806803Z","iopub.execute_input":"2023-04-06T15:52:34.807215Z","iopub.status.idle":"2023-04-06T15:52:34.815532Z","shell.execute_reply.started":"2023-04-06T15:52:34.807177Z","shell.execute_reply":"2023-04-06T15:52:34.813320Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1WfnmYuP58qC_68dKIcYgqqbPc_Usm6yU","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:52:34.937655Z","iopub.execute_input":"2023-04-06T15:52:34.937954Z","iopub.status.idle":"2023-04-06T15:52:39.304692Z","shell.execute_reply.started":"2023-04-06T15:52:34.937925Z","shell.execute_reply":"2023-04-06T15:52:39.303306Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1WfnmYuP58qC_68dKIcYgqqbPc_Usm6yU\nTo: /kaggle/working/um_uh_1.wav\n100%|██████████████████████████████████████| 41.7k/41.7k [00:00<00:00, 4.08MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def speech_file_to_array_fn(file_path):\n    speech_array, sampling_rate = torchaudio.load(file_path)\n    speech_array = speech_array.squeeze().numpy()\n    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n    return speech_array\n\n\ndef predict(speech_array):\n    features = processor(speech_array, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True, return_attention_mask=True)\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n    \n    with torch.no_grad():\n        logits = model(input_values, attention_mask).logits\n\n    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n    return pred_ids","metadata":{"execution":{"iopub.status.busy":"2023-04-06T17:00:03.327525Z","iopub.execute_input":"2023-04-06T17:00:03.328676Z","iopub.status.idle":"2023-04-06T17:00:03.338285Z","shell.execute_reply.started":"2023-04-06T17:00:03.328628Z","shell.execute_reply":"2023-04-06T17:00:03.337237Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"speech_arr = speech_file_to_array_fn(\"um_uh_1.wav\")","metadata":{"execution":{"iopub.status.busy":"2023-04-06T17:00:03.675654Z","iopub.execute_input":"2023-04-06T17:00:03.675998Z","iopub.status.idle":"2023-04-06T17:00:03.685668Z","shell.execute_reply.started":"2023-04-06T17:00:03.675967Z","shell.execute_reply":"2023-04-06T17:00:03.684386Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"predict(speech_arr)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T17:56:00.326846Z","iopub.execute_input":"2023-04-06T17:56:00.327222Z","iopub.status.idle":"2023-04-06T17:56:00.371434Z","shell.execute_reply.started":"2023-04-06T17:56:00.327189Z","shell.execute_reply":"2023-04-06T17:56:00.368355Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"array([3])"},"metadata":{}}]},{"cell_type":"code","source":"# try to disable wandb to run on kaggle automatically\n# import os\n# os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:34:46.895335Z","iopub.execute_input":"2023-04-06T15:34:46.896332Z","iopub.status.idle":"2023-04-06T15:34:46.904024Z","shell.execute_reply.started":"2023-04-06T15:34:46.896250Z","shell.execute_reply":"2023-04-06T15:34:46.902469Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Save and load model","metadata":{}},{"cell_type":"code","source":"# saving model\ntorch.save(model.state_dict(), 'final_clf.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:34:50.703378Z","iopub.execute_input":"2023-04-06T15:34:50.703740Z","iopub.status.idle":"2023-04-06T15:34:53.447439Z","shell.execute_reply.started":"2023-04-06T15:34:50.703708Z","shell.execute_reply":"2023-04-06T15:34:53.445140Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model_loaded = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:26:29.898005Z","iopub.execute_input":"2023-04-06T16:26:29.898487Z","iopub.status.idle":"2023-04-06T16:26:50.783235Z","shell.execute_reply.started":"2023-04-06T16:26:29.898449Z","shell.execute_reply":"2023-04-06T16:26:50.782045Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_loaded.load_state_dict(torch.load('final_clf.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:26:50.785287Z","iopub.execute_input":"2023-04-06T16:26:50.786052Z","iopub.status.idle":"2023-04-06T16:27:07.621667Z","shell.execute_reply.started":"2023-04-06T16:26:50.786010Z","shell.execute_reply":"2023-04-06T16:27:07.617773Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:00:32.248080Z","iopub.execute_input":"2023-04-06T16:00:32.251089Z","iopub.status.idle":"2023-04-06T16:00:32.273775Z","shell.execute_reply.started":"2023-04-06T16:00:32.251047Z","shell.execute_reply":"2023-04-06T16:00:32.268743Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model_loaded.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loaded.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_loaded.state_dict(), 'interjections_clf_cpu.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:27:13.692119Z","iopub.execute_input":"2023-04-06T16:27:13.692504Z","iopub.status.idle":"2023-04-06T16:27:16.115384Z","shell.execute_reply.started":"2023-04-06T16:27:13.692471Z","shell.execute_reply":"2023-04-06T16:27:16.113954Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"#experiment","metadata":{"execution":{"iopub.status.busy":"2023-04-06T09:54:25.782053Z","iopub.execute_input":"2023-04-06T09:54:25.784741Z","iopub.status.idle":"2023-04-06T09:54:25.792255Z","shell.execute_reply.started":"2023-04-06T09:54:25.784698Z","shell.execute_reply":"2023-04-06T09:54:25.791282Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# saving model\ntorch.save(model.state_dict(), 'final_clf_test.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:00:55.818233Z","iopub.execute_input":"2023-04-06T16:00:55.818632Z","iopub.status.idle":"2023-04-06T16:00:57.970698Z","shell.execute_reply.started":"2023-04-06T16:00:55.818588Z","shell.execute_reply":"2023-04-06T16:00:57.969333Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"model_loaded_test = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:00:57.972808Z","iopub.execute_input":"2023-04-06T16:00:57.973421Z","iopub.status.idle":"2023-04-06T16:01:03.608034Z","shell.execute_reply.started":"2023-04-06T16:00:57.973383Z","shell.execute_reply":"2023-04-06T16:01:03.606570Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_loaded_test.classifier.out_proj.weight","metadata":{"execution":{"iopub.status.busy":"2023-04-01T16:09:18.668969Z","iopub.execute_input":"2023-04-01T16:09:18.669723Z","iopub.status.idle":"2023-04-01T16:09:18.683563Z","shell.execute_reply.started":"2023-04-01T16:09:18.669681Z","shell.execute_reply":"2023-04-01T16:09:18.682580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loaded_test.load_state_dict(torch.load('final_clf_test.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T16:09:18.684928Z","iopub.execute_input":"2023-04-01T16:09:18.685596Z","iopub.status.idle":"2023-04-01T16:09:26.495428Z","shell.execute_reply.started":"2023-04-01T16:09:18.685560Z","shell.execute_reply":"2023-04-01T16:09:26.491936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_loaded_test.classifier.out_proj.weight","metadata":{"execution":{"iopub.status.busy":"2023-04-01T15:59:52.492167Z","iopub.execute_input":"2023-04-01T15:59:52.493519Z","iopub.status.idle":"2023-04-01T15:59:52.507873Z","shell.execute_reply.started":"2023-04-01T15:59:52.493475Z","shell.execute_reply":"2023-04-01T15:59:52.506552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.classifier.out_proj.weight","metadata":{"execution":{"iopub.status.busy":"2023-04-01T16:00:51.666592Z","iopub.execute_input":"2023-04-01T16:00:51.667191Z","iopub.status.idle":"2023-04-01T16:00:51.681914Z","shell.execute_reply.started":"2023-04-01T16:00:51.667144Z","shell.execute_reply":"2023-04-01T16:00:51.680903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"VsaOTx_FVm0i"}},{"cell_type":"code","source":"import librosa\nfrom sklearn.metrics import classification_report","metadata":{"id":"4tGNY7hRXO44","execution":{"iopub.status.busy":"2023-04-06T16:01:56.393501Z","iopub.execute_input":"2023-04-06T16:01:56.394226Z","iopub.status.idle":"2023-04-06T16:01:56.402112Z","shell.execute_reply.started":"2023-04-06T16:01:56.394188Z","shell.execute_reply":"2023-04-06T16:01:56.400758Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files={\"test\": \"content/data/test.csv\"}, delimiter=\"\\t\")[\"test\"]\ntest_dataset","metadata":{"id":"IYxg1Tfo2VUw","outputId":"1bdf2b50-7663-4c63-eb16-bcd97614f285","execution":{"iopub.status.busy":"2023-04-06T16:01:56.623306Z","iopub.execute_input":"2023-04-06T16:01:56.623873Z","iopub.status.idle":"2023-04-06T16:01:57.497346Z","shell.execute_reply.started":"2023-04-06T16:01:56.623823Z","shell.execute_reply":"2023-04-06T16:01:57.496137Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ab68ac97e14a519c801f19c4750351"}},"metadata":{}},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['path', 'label'],\n    num_rows: 1568\n})"},"metadata":{}}]},{"cell_type":"code","source":"# model_loaded\n","metadata":{"id":"QgZFkMDHW_Um","outputId":"8efb8a2c-2021-4b76-9ba6-d6a0d7c20dde","execution":{"iopub.status.busy":"2023-04-06T16:01:57.499668Z","iopub.execute_input":"2023-04-06T16:01:57.500374Z","iopub.status.idle":"2023-04-06T16:01:57.509058Z","shell.execute_reply.started":"2023-04-06T16:01:57.500327Z","shell.execute_reply":"2023-04-06T16:01:57.507538Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# model_name_or_path = \"facebook/wav2vec2-base-960h\"\nmodel_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\n# config = AutoConfig.from_pretrained(model_name_or_path)\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n# model_name_or_path = \"final_clf.pt\"\n# model_new_loaded = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)","metadata":{"id":"-ESFEXeaWgua","outputId":"1ece0dda-8d00-4b24-d227-d787b2dcbb61","execution":{"iopub.status.busy":"2023-04-06T16:01:57.510962Z","iopub.execute_input":"2023-04-06T16:01:57.511438Z","iopub.status.idle":"2023-04-06T16:01:58.534539Z","shell.execute_reply.started":"2023-04-06T16:01:57.511405Z","shell.execute_reply":"2023-04-06T16:01:58.533333Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    speech_array = speech_array.squeeze().numpy()\n    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n\n    batch[\"speech\"] = speech_array\n    return batch\n\n\ndef predict(batch):\n    features = processor(batch[\"speech\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True, return_attention_mask=True)\n\n#     print(features)\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n    \n    with torch.no_grad():\n        logits = model(input_values, attention_mask).logits #attention_mask\n\n    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n    batch[\"predicted\"] = pred_ids\n    return batch","metadata":{"id":"BkEd4w8IV7kZ","execution":{"iopub.status.busy":"2023-04-06T16:01:58.537086Z","iopub.execute_input":"2023-04-06T16:01:58.537770Z","iopub.status.idle":"2023-04-06T16:01:58.551007Z","shell.execute_reply.started":"2023-04-06T16:01:58.537730Z","shell.execute_reply":"2023-04-06T16:01:58.549293Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_dataset.map(speech_file_to_array_fn)","metadata":{"id":"S4P6P6XwW85p","outputId":"59cd6b44-182f-438a-f32d-a604d96132c5","execution":{"iopub.status.busy":"2023-04-06T16:01:58.553830Z","iopub.execute_input":"2023-04-06T16:01:58.555479Z","iopub.status.idle":"2023-04-06T16:02:00.697571Z","shell.execute_reply.started":"2023-04-06T16:01:58.555441Z","shell.execute_reply":"2023-04-06T16:02:00.696306Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1568 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747b49161f954b5ba4391befb840f94c"}},"metadata":{}}]},{"cell_type":"code","source":"test_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-06T16:02:00.703222Z","iopub.execute_input":"2023-04-06T16:02:00.705892Z","iopub.status.idle":"2023-04-06T16:02:00.717828Z","shell.execute_reply.started":"2023-04-06T16:02:00.705849Z","shell.execute_reply":"2023-04-06T16:02:00.716663Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['path', 'label', 'speech'],\n    num_rows: 1568\n})"},"metadata":{}}]},{"cell_type":"code","source":"result = test_dataset.map(predict, batched=True, batch_size=8)","metadata":{"id":"K_oZJzHsXKHv","outputId":"6bf8dd80-e5db-4415-810b-bfa2fa76e60f","execution":{"iopub.status.busy":"2023-04-06T16:02:00.721853Z","iopub.execute_input":"2023-04-06T16:02:00.722344Z","iopub.status.idle":"2023-04-06T16:02:14.984260Z","shell.execute_reply.started":"2023-04-06T16:02:00.722306Z","shell.execute_reply":"2023-04-06T16:02:14.982885Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/196 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe38f9ee7fe4105b6970a20fab448c9"}},"metadata":{}}]},{"cell_type":"code","source":"label_names = [config.id2label[i] for i in range(config.num_labels)]\nlabel_names","metadata":{"id":"BnfJLZvAaxTo","outputId":"dfea7223-1211-471e-8653-5fef695e58fb","execution":{"iopub.status.busy":"2023-04-06T16:02:14.989519Z","iopub.execute_input":"2023-04-06T16:02:14.992148Z","iopub.status.idle":"2023-04-06T16:02:15.004966Z","shell.execute_reply.started":"2023-04-06T16:02:14.992106Z","shell.execute_reply":"2023-04-06T16:02:15.003968Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"['Breath', 'Laughter', 'Music', 'Uh', 'Um', 'Words']"},"metadata":{}}]},{"cell_type":"code","source":"y_true = [config.label2id[name] for name in result[\"label\"]]\ny_pred = result[\"predicted\"]\n\nprint(y_true[:5])\nprint(y_pred[:5])","metadata":{"id":"vRtajzvTabeH","outputId":"6a3ea32f-bc88-4b50-de82-ef96feb5d6be","execution":{"iopub.status.busy":"2023-04-06T16:02:15.009929Z","iopub.execute_input":"2023-04-06T16:02:15.011481Z","iopub.status.idle":"2023-04-06T16:02:15.097468Z","shell.execute_reply.started":"2023-04-06T16:02:15.011444Z","shell.execute_reply":"2023-04-06T16:02:15.095155Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"[0, 5, 0, 4, 4]\n[0, 5, 0, 4, 4]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(classification_report(y_true, y_pred, target_names=label_names))","metadata":{"id":"tUt5rIppXrzl","outputId":"b434aa90-93f5-45bc-80bf-a821c9dff7c7","execution":{"iopub.status.busy":"2023-04-06T16:02:15.099029Z","iopub.execute_input":"2023-04-06T16:02:15.099755Z","iopub.status.idle":"2023-04-06T16:02:15.121369Z","shell.execute_reply.started":"2023-04-06T16:02:15.099718Z","shell.execute_reply":"2023-04-06T16:02:15.120221Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n      Breath       0.67      0.64      0.65        84\n    Laughter       0.75      0.62      0.68        74\n       Music       0.94      0.73      0.82       138\n          Uh       0.76      0.64      0.69       413\n          Um       0.85      0.83      0.84       519\n       Words       0.62      0.84      0.71       340\n\n    accuracy                           0.75      1568\n   macro avg       0.76      0.72      0.73      1568\nweighted avg       0.77      0.75      0.76      1568\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# precision    recall  f1-score   support\n\n#       Breath       0.79      0.89      0.84        84\n#     Laughter       0.78      0.85      0.81        74\n#        Music       0.95      0.87      0.91       138\n#           Uh       0.92      0.80      0.85       413\n#           Um       0.92      0.95      0.93       519\n#        Words       0.80      0.88      0.84       340\n\n#     accuracy                           0.88      1568\n#    macro avg       0.86      0.87      0.86      1568\n# weighted avg       0.88      0.88      0.88      1568\n\n\n#  precision    recall  f1-score   support\n\n#       Breath       0.65      0.62      0.63        84\n#     Laughter       0.77      0.62      0.69        74\n#        Music       0.92      0.76      0.83       138\n#           Uh       0.74      0.64      0.69       413\n#           Um       0.85      0.81      0.83       519\n#        Words       0.59      0.81      0.68       340\n\n#     accuracy                           0.74      1568\n#    macro avg       0.75      0.71      0.73      1568\n# weighted avg       0.76      0.74      0.74      1568","metadata":{"execution":{"iopub.status.busy":"2023-04-01T15:52:10.098458Z","iopub.execute_input":"2023-04-01T15:52:10.099288Z","iopub.status.idle":"2023-04-01T15:52:10.105948Z","shell.execute_reply.started":"2023-04-01T15:52:10.099245Z","shell.execute_reply":"2023-04-01T15:52:10.103499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{"id":"Ylb2Z6Xke2ro"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom transformers import AutoConfig, Wav2Vec2Processor\n\nimport librosa\nimport IPython.display as ipd\nimport numpy as np\nimport pandas as pd","metadata":{"id":"EQzCioPhWIiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name_or_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\nconfig = AutoConfig.from_pretrained(model_name_or_path)\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\nsampling_rate = processor.feature_extractor.sampling_rate\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)","metadata":{"id":"DocavTvQWIr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def speech_file_to_array_fn(path, sampling_rate):\n    speech_array, _sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\n\ndef predict(path, sampling_rate):\n    speech = speech_file_to_array_fn(path, sampling_rate)\n    features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n\n    with torch.no_grad():\n        logits = model(input_values, attention_mask=attention_mask).logits\n\n    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n    return outputs\n\n\nSTYLES = \"\"\"\n<style>\ndiv.display_data {\n    margin: 0 auto;\n    max-width: 500px;\n}\ntable.xxx {\n    margin: 50px !important;\n    float: right !important;\n    clear: both !important;\n}\ntable.xxx td {\n    min-width: 300px !important;\n    text-align: center !important;\n}\n</style>\n\"\"\".strip()\n\ndef prediction(df_row):\n    path, emotion = df_row[\"path\"], df_row[\"emotion\"]\n    df = pd.DataFrame([{\"Emotion\": emotion, \"Sentence\": \"    \"}])\n    setup = {\n        'border': 2,\n        'show_dimensions': True,\n        'justify': 'center',\n        'classes': 'xxx',\n        'escape': False,\n    }\n    ipd.display(ipd.HTML(STYLES + df.to_html(**setup) + \"<br />\"))\n    speech, sr = torchaudio.load(path)\n    speech = speech[0].numpy().squeeze()\n    speech = librosa.resample(np.asarray(speech), sr, sampling_rate)\n    ipd.display(ipd.Audio(data=np.asarray(speech), autoplay=True, rate=sampling_rate))\n\n    outputs = predict(path, sampling_rate)\n    r = pd.DataFrame(outputs)\n    ipd.display(ipd.HTML(STYLES + r.to_html(**setup) + \"<br />\"))","metadata":{"id":"1SSs95o9WIvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/content/data/test.csv\", sep=\"\\t\")\ntest.head()","metadata":{"id":"UD7oUP20YwYT","outputId":"ec5251d3-4e27-42e7-f8de-54544c3beb2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(test.iloc[0])","metadata":{"id":"FlJO2LfVWIyT","outputId":"74a524ce-f4b5-46a9-df20-dc8be43a0a4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(test.iloc[1])","metadata":{"id":"hzoKOgpoWI1K","outputId":"0fad7fac-2f09-455f-c5b3-30316911cf54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(test.iloc[2])","metadata":{"id":"7nqwbTTXSfMO","outputId":"ab767a32-1c73-4eed-f388-d12e5f4a0fcf"},"execution_count":null,"outputs":[]}]}